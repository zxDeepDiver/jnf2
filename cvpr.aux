\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{russakovsky2015imagenet}
\citation{lin2014microsoft}
\citation{Cordts2016Cityscapes}
\citation{he2016deep}
\citation{denil2013predicting}
\citation{li2020group}
\citation{jacob2018quantization}
\citation{hinton2015distilling}
\citation{denton2014exploiting}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{introduction}{{1}{1}{\hskip -1em.~Introduction}{section.1}{}}
\@writefile{brf}{\backcite{russakovsky2015imagenet}{{1}{1}{figure.2}}}
\@writefile{brf}{\backcite{lin2014microsoft}{{1}{1}{figure.2}}}
\@writefile{brf}{\backcite{Cordts2016Cityscapes}{{1}{1}{figure.2}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Visual comparison between the previous sparsity-training-based methods and the proposed MaskSparsity method.}}{1}{figure.1}\protected@file@percent }
\newlabel{MAskSparsit}{{1}{1}{Visual comparison between the previous sparsity-training-based methods and the proposed MaskSparsity method}{figure.1}{}}
\@writefile{brf}{\backcite{he2016deep}{{1}{1}{figure.2}}}
\@writefile{brf}{\backcite{denil2013predicting}{{1}{1}{figure.2}}}
\@writefile{brf}{\backcite{li2020group}{{1}{1}{figure.2}}}
\@writefile{brf}{\backcite{jacob2018quantization}{{1}{1}{figure.2}}}
\@writefile{brf}{\backcite{hinton2015distilling}{{1}{1}{figure.2}}}
\@writefile{brf}{\backcite{denton2014exploiting}{{1}{1}{figure.2}}}
\citation{alvarez2016learning}
\citation{wen2016learning}
\citation{huang2018data}
\citation{liu2017learning}
\citation{alvarez2016learning}
\citation{wen2016learning}
\citation{alvarez2017compression}
\citation{OICSROS}
\citation{Hinge}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The pipeline of the proposed MaskSparsity method.}}{2}{figure.2}\protected@file@percent }
\newlabel{MAskSparsityPipeline}{{2}{2}{The pipeline of the proposed MaskSparsity method}{figure.2}{}}
\@writefile{brf}{\backcite{alvarez2016learning,wen2016learning}{{2}{1}{figure.2}}}
\@writefile{brf}{\backcite{huang2018data,liu2017learning}{{2}{1}{figure.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related Work}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\hskip -1em.\nobreakspace  {}Sparsity-training-based Pruning Methods}{2}{subsection.2.1}\protected@file@percent }
\citation{liu2017learning}
\citation{huang2018data}
\citation{Srinivas2017TrainingSN}
\citation{OT}
\citation{Dependency}
\citation{L1}
\citation{GM}
\citation{FPGM}
\citation{lin2020hrank}
\citation{ioffe2015batch}
\citation{he2016deep}
\citation{ye2020channel}
\@writefile{brf}{\backcite{alvarez2016learning}{{3}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{wen2016learning}{{3}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{alvarez2017compression}{{3}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{OICSROS}{{3}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{Hinge}{{3}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{liu2017learning}{{3}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{huang2018data}{{3}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{Srinivas2017TrainingSN}{{3}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{OT}{{3}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{Dependency}{{3}{2.1}{subsection.2.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\hskip -1em.\nobreakspace  {}Non-sparsity-training-based Pruning Methods}{3}{subsection.2.2}\protected@file@percent }
\@writefile{brf}{\backcite{L1}{{3}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{GM}{{3}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{FPGM}{{3}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{lin2020hrank}{{3}{2.2}{subsection.2.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Methodology}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Notations and background}{3}{subsection.3.1}\protected@file@percent }
\newlabel{Notations-and-background}{{3.1}{3}{\hskip -1em.~Notations and background}{subsection.3.1}{}}
\@writefile{brf}{\backcite{ioffe2015batch}{{3}{3.1}{subsection.3.1}}}
\newlabel{eq-conv-bn}{{1}{3}{\hskip -1em.~Notations and background}{equation.3.1}{}}
\newlabel{eq-tra-sp}{{2}{3}{\hskip -1em.~Notations and background}{equation.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {} Analysis of Previous Sparsity-training-based Methods }{4}{subsection.3.2}\protected@file@percent }
\newlabel{Analysisoftraditionalsp}{{3.2}{4}{\hskip -1em.~ Analysis of Previous Sparsity-training-based Methods}{subsection.3.2}{}}
\@writefile{brf}{\backcite{he2016deep}{{4}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{ye2020channel}{{4}{3.2}{subsection.3.2}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Distribution of scaling factors of ResNet50 before and after global sparsity training.}}{4}{figure.3}\protected@file@percent }
\newlabel{pbm}{{3}{4}{Distribution of scaling factors of ResNet50 before and after global sparsity training}{figure.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Algorithm Description of MaskSparsity}}{4}{algorithm.1}\protected@file@percent }
\newlabel{alg:FPGM}{{1}{4}{\hskip -1em.~ Analysis of Previous Sparsity-training-based Methods}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}MaskSparsity for Fine-grained Sparse Regularization}{4}{subsection.3.3}\protected@file@percent }
\newlabel{MaskSP}{{3.3}{4}{\hskip -1em.~MaskSparsity for Fine-grained Sparse Regularization}{subsection.3.3}{}}
\citation{he2016deep}
\citation{liu2017learning}
\citation{OT}
\citation{he2018soft}
\citation{GAL}
\citation{HRank}
\citation{Hinge}
\citation{xu2018hybrid}
\citation{Metapruning}
\citation{luo2018autopruner}
\citation{FPGM}
\citation{zhuang2018discrimination}
\citation{DBLP:journals/pami/LuoZZXWL19}
\citation{HRank}
\citation{krizhevsky2009learning}
\citation{russakovsky2015imagenet}
\citation{He2016IdentityMI}
\citation{he2016deep}
\citation{he2016deep}
\citation{he2016deep}
\newlabel{maskgeneration}{{3}{5}{\hskip -1em.~MaskSparsity for Fine-grained Sparse Regularization}{equation.3.3}{}}
\newlabel{eq-Mask-sp}{{4}{5}{\hskip -1em.~MaskSparsity for Fine-grained Sparse Regularization}{equation.3.4}{}}
\newlabel{Pruning with Residual-Connections}{{3.3}{5}{\hskip -1em.~MaskSparsity for Fine-grained Sparse Regularization}{equation.3.4}{}}
\@writefile{brf}{\backcite{he2016deep}{{5}{3.3}{equation.3.4}}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Experiments}{5}{section.4}\protected@file@percent }
\newlabel{Experiment}{{4}{5}{\hskip -1em.~Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.\nobreakspace  {}Experimental Settings}{5}{subsection.4.1}\protected@file@percent }
\@writefile{brf}{\backcite{krizhevsky2009learning}{{5}{4.1}{subsection.4.1}}}
\@writefile{brf}{\backcite{russakovsky2015imagenet}{{5}{4.1}{subsection.4.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  An illustration of pruning with Residual-Connections. (a) shows the structure of one typical network stage. Only the last convolution layers of each ResBlock is highlighted. (b) shows the pruning strategy of the previous sparsity-training-based method and MaskSparsity method. The shallow blue boxes mean important channels. The deep blue boxes mean the unimportant channels. The red cross means that the channel can be pruned. }}{5}{figure.4}\protected@file@percent }
\newlabel{Pruningmask-method}{{4}{5}{An illustration of pruning with Residual-Connections. (a) shows the structure of one typical network stage. Only the last convolution layers of each ResBlock is highlighted. (b) shows the pruning strategy of the previous sparsity-training-based method and MaskSparsity method. The shallow blue boxes mean important channels. The deep blue boxes mean the unimportant channels. The red cross means that the channel can be pruned}{figure.4}{}}
\@writefile{brf}{\backcite{He2016IdentityMI}{{5}{4.1}{subsection.4.1}}}
\@writefile{brf}{\backcite{he2016deep}{{5}{4.1}{subsection.4.1}}}
\@writefile{brf}{\backcite{he2016deep}{{5}{4.1}{subsection.4.1}}}
\@writefile{brf}{\backcite{he2016deep}{{5}{4.1}{subsection.4.1}}}
\citation{liu2017learning}
\citation{OT}
\citation{FPGM}
\citation{zhuang2018discrimination}
\citation{Metapruning}
\citation{FPGM}
\citation{Hinge}
\citation{FPGM}
\citation{Hinge}
\citation{HRank}
\citation{FPGM}
\citation{CSGD}
\citation{yu2018nisp}
\citation{Hinge}
\citation{he2017channel}
\citation{he2018amc}
\citation{he2017channel}
\citation{LeGR}
\citation{FPGM}
\citation{he2020learning}
\citation{GAL}
\citation{HRank}
\citation{li2016pruning}
\citation{he2018soft}
\citation{yu2018nisp}
\citation{GAL}
\citation{FPGM}
\citation{HRank}
\citation{he2020learning}
\citation{HRank}
\citation{CSGD}
\citation{SASL}
\citation{liu2017learning}
\citation{FPGM}
\citation{li2016pruning}
\citation{liu2017learning}
\citation{li2016pruning}
\citation{FPGM}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Evaluation results using ResNet-50 on ILSVRC-2012.}}{6}{table.1}\protected@file@percent }
\@writefile{brf}{\backcite{liu2017learning}{{6}{1}{table.1}}}
\@writefile{brf}{\backcite{OT}{{6}{1}{table.1}}}
\@writefile{brf}{\backcite{he2018soft}{{6}{1}{table.1}}}
\@writefile{brf}{\backcite{GAL}{{6}{1}{table.1}}}
\@writefile{brf}{\backcite{HRank}{{6}{1}{table.1}}}
\@writefile{brf}{\backcite{Hinge}{{6}{1}{table.1}}}
\@writefile{brf}{\backcite{xu2018hybrid}{{6}{1}{table.1}}}
\@writefile{brf}{\backcite{Metapruning}{{6}{1}{table.1}}}
\@writefile{brf}{\backcite{luo2018autopruner}{{6}{1}{table.1}}}
\@writefile{brf}{\backcite{FPGM}{{6}{1}{table.1}}}
\@writefile{brf}{\backcite{zhuang2018discrimination}{{6}{1}{table.1}}}
\@writefile{brf}{\backcite{DBLP:journals/pami/LuoZZXWL19}{{6}{1}{table.1}}}
\@writefile{brf}{\backcite{HRank}{{6}{1}{table.1}}}
\newlabel{exp-table-imgnet}{{1}{6}{Evaluation results using ResNet-50 on ILSVRC-2012}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em.\nobreakspace  {}Results and Analysis}{6}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Results on ILSVRC-2012}{6}{subsubsection.4.2.1}\protected@file@percent }
\newlabel{results_on_ILSVRC-2012}{{4.2.1}{6}{Results on ILSVRC-2012}{subsubsection.4.2.1}{}}
\@writefile{brf}{\backcite{liu2017learning}{{6}{4.2.1}{subsubsection.4.2.1}}}
\@writefile{brf}{\backcite{OT}{{6}{4.2.1}{subsubsection.4.2.1}}}
\@writefile{brf}{\backcite{FPGM}{{6}{4.2.1}{subsubsection.4.2.1}}}
\@writefile{brf}{\backcite{zhuang2018discrimination}{{6}{4.2.1}{subsubsection.4.2.1}}}
\@writefile{brf}{\backcite{Metapruning}{{6}{4.2.1}{subsubsection.4.2.1}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Results on CIFAR-10}{6}{subsubsection.4.2.2}\protected@file@percent }
\newlabel{results_on_cifar10}{{4.2.2}{6}{Results on CIFAR-10}{subsubsection.4.2.2}{}}
\@writefile{brf}{\backcite{FPGM}{{6}{4.2.2}{subsubsection.4.2.2}}}
\@writefile{brf}{\backcite{Hinge}{{6}{4.2.2}{subsubsection.4.2.2}}}
\@writefile{brf}{\backcite{FPGM}{{6}{4.2.2}{subsubsection.4.2.2}}}
\@writefile{brf}{\backcite{Hinge}{{6}{4.2.2}{subsubsection.4.2.2}}}
\@writefile{brf}{\backcite{HRank}{{6}{4.2.2}{subsubsection.4.2.2}}}
\@writefile{brf}{\backcite{FPGM}{{6}{4.2.2}{subsubsection.4.2.2}}}
\@writefile{brf}{\backcite{CSGD}{{6}{4.2.2}{subsubsection.4.2.2}}}
\citation{li2016pruning}
\citation{FPGM}
\citation{FPGM}
\citation{liu2017learning}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Evaluation results using ResNet-56 on CIFAR-10.}}{7}{table.2}\protected@file@percent }
\@writefile{brf}{\backcite{yu2018nisp}{{7}{2}{table.2}}}
\@writefile{brf}{\backcite{Hinge}{{7}{2}{table.2}}}
\@writefile{brf}{\backcite{he2017channel}{{7}{2}{table.2}}}
\@writefile{brf}{\backcite{he2018amc}{{7}{2}{table.2}}}
\@writefile{brf}{\backcite{he2017channel}{{7}{2}{table.2}}}
\@writefile{brf}{\backcite{LeGR}{{7}{2}{table.2}}}
\@writefile{brf}{\backcite{FPGM}{{7}{2}{table.2}}}
\@writefile{brf}{\backcite{he2020learning}{{7}{2}{table.2}}}
\@writefile{brf}{\backcite{GAL}{{7}{2}{table.2}}}
\@writefile{brf}{\backcite{HRank}{{7}{2}{table.2}}}
\newlabel{exp-table-cifar10-r56}{{2}{7}{Evaluation results using ResNet-56 on CIFAR-10}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Evaluation results using ResNet-110 on CIFAR-10.}}{7}{table.3}\protected@file@percent }
\@writefile{brf}{\backcite{li2016pruning}{{7}{3}{table.3}}}
\@writefile{brf}{\backcite{he2018soft}{{7}{3}{table.3}}}
\@writefile{brf}{\backcite{yu2018nisp}{{7}{3}{table.3}}}
\@writefile{brf}{\backcite{GAL}{{7}{3}{table.3}}}
\@writefile{brf}{\backcite{FPGM}{{7}{3}{table.3}}}
\@writefile{brf}{\backcite{HRank}{{7}{3}{table.3}}}
\@writefile{brf}{\backcite{he2020learning}{{7}{3}{table.3}}}
\@writefile{brf}{\backcite{HRank}{{7}{3}{table.3}}}
\@writefile{brf}{\backcite{CSGD}{{7}{3}{table.3}}}
\@writefile{brf}{\backcite{SASL}{{7}{3}{table.3}}}
\newlabel{exp-table-cifar10-r110}{{3}{7}{Evaluation results using ResNet-110 on CIFAR-10}{table.3}{}}
\@writefile{brf}{\backcite{liu2017learning}{{7}{4.2.2}{table.3}}}
\@writefile{brf}{\backcite{FPGM}{{7}{4.2.2}{table.3}}}
\@writefile{brf}{\backcite{li2016pruning}{{7}{4.2.2}{table.3}}}
\@writefile{brf}{\backcite{liu2017learning}{{7}{4.2.2}{table.3}}}
\@writefile{brf}{\backcite{li2016pruning}{{7}{4.2.2}{table.3}}}
\@writefile{brf}{\backcite{FPGM}{{7}{4.2.2}{table.3}}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Evaluation results using VGG-16 on CIFAR-10.}}{7}{table.4}\protected@file@percent }
\@writefile{brf}{\backcite{li2016pruning}{{7}{4}{table.4}}}
\@writefile{brf}{\backcite{FPGM}{{7}{4}{table.4}}}
\@writefile{brf}{\backcite{FPGM}{{7}{4}{table.4}}}
\@writefile{brf}{\backcite{liu2017learning}{{7}{4}{table.4}}}
\newlabel{exp-table-cifar10-VGG16}{{4}{7}{Evaluation results using VGG-16 on CIFAR-10}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}\hskip -1em.\nobreakspace  {}Ablation Study}{7}{subsection.4.3}\protected@file@percent }
\newlabel{abalation}{{4.3}{7}{\hskip -1em.~Ablation Study}{subsection.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Distribution of scaling factors of ResNet50 on ILSVRC-2012 before and after the MaskSparsity's sparsity training.}}{8}{figure.5}\protected@file@percent }
\newlabel{dstbtion-mask-sp}{{5}{8}{Distribution of scaling factors of ResNet50 on ILSVRC-2012 before and after the MaskSparsity's sparsity training}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The gradients of the scaling factors of a certain channel over the additional 1, 000 sparsity-training iterations after the ending of the sparsity training stage. The spare regularization keeps the same as that used in the original sparsity training. We choose an important channel and an unimportant channel to conduct this experiment. (a) The important channel, global sparse regularization. (b) The unimportant channel, global sparse regularization. (c) The important channel, fine-grained sparse regularization of MaskSparsity. (d) The unimportant channel, fine-grained sparse regularization of MaskSparsity.}}{8}{figure.6}\protected@file@percent }
\newlabel{grad}{{6}{8}{The gradients of the scaling factors of a certain channel over the additional 1, 000 sparsity-training iterations after the ending of the sparsity training stage. The spare regularization keeps the same as that used in the original sparsity training. We choose an important channel and an unimportant channel to conduct this experiment. (a) The important channel, global sparse regularization. (b) The unimportant channel, global sparse regularization. (c) The important channel, fine-grained sparse regularization of MaskSparsity. (d) The unimportant channel, fine-grained sparse regularization of MaskSparsity}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  The unimportant-channel masks in every last layer of ResBlocks in the third and fourth network stage. The data were obtained from ResNet50 trained with traditional sparsity-training-based methods and MaskSparsity on ILSVRC-2012. }}{8}{figure.7}\protected@file@percent }
\newlabel{Pruning-mask-exp}{{7}{8}{The unimportant-channel masks in every last layer of ResBlocks in the third and fourth network stage. The data were obtained from ResNet50 trained with traditional sparsity-training-based methods and MaskSparsity on ILSVRC-2012}{figure.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The stage-wise performance in the case of pruning ResNet-56 on CIFAR-10.}}{8}{table.5}\protected@file@percent }
\newlabel{Finetune_scratch_cifar10}{{5}{8}{The stage-wise performance in the case of pruning ResNet-56 on CIFAR-10}{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Pruning ResNet56 on CIFAR-10.}}{8}{table.6}\protected@file@percent }
\newlabel{pruning_threshold}{{6}{8}{Pruning ResNet56 on CIFAR-10}{table.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Conclusion}{8}{section.5}\protected@file@percent }
\newlabel{Conclusion}{{5}{8}{\hskip -1em.~Conclusion}{section.5}{}}
\bibstyle{ieee_fullname}
\bibdata{egbib}
\bibcite{alvarez2016learning}{1}
\bibcite{alvarez2017compression}{2}
\bibcite{LeGR}{3}
\bibcite{Cordts2016Cityscapes}{4}
\bibcite{denil2013predicting}{5}
\bibcite{denton2014exploiting}{6}
\bibcite{CSGD}{7}
\bibcite{GM}{8}
\bibcite{he2016deep}{9}
\bibcite{He2016IdentityMI}{10}
\bibcite{he2020learning}{11}
\bibcite{he2018soft}{12}
\bibcite{he2018amc}{13}
\bibcite{FPGM}{14}
\bibcite{he2017channel}{15}
\bibcite{hinton2015distilling}{16}
\bibcite{huang2018data}{17}
\bibcite{ioffe2015batch}{18}
\bibcite{jacob2018quantization}{19}
\bibcite{krizhevsky2009learning}{20}
\bibcite{L1}{21}
\bibcite{li2016pruning}{22}
\bibcite{OICSROS}{23}
\bibcite{Hinge}{24}
\bibcite{li2020group}{25}
\bibcite{lin2020hrank}{26}
\bibcite{HRank}{27}
\bibcite{GAL}{28}
\bibcite{lin2014microsoft}{29}
\bibcite{liu2017learning}{30}
\bibcite{Metapruning}{31}
\bibcite{DBLP:journals/pami/LuoZZXWL19}{32}
\bibcite{luo2018autopruner}{33}
\bibcite{russakovsky2015imagenet}{34}
\bibcite{SASL}{35}
\bibcite{Srinivas2017TrainingSN}{36}
\bibcite{wen2016learning}{37}
\bibcite{xu2018hybrid}{38}
\bibcite{OT}{39}
\bibcite{ye2020channel}{40}
\bibcite{yu2018nisp}{41}
\bibcite{Dependency}{42}
\bibcite{zhuang2018discrimination}{43}
